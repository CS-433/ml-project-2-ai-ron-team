{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7f0fa58",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/wb/mz3kpqv55ksb706w7v7bvg200000gn/T/ipykernel_23152/3772588110.py:107: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from pandas.api.types import is_numeric_dtype\n",
    "\n",
    "def one_hot_encode_columns(df, columns):\n",
    "    \"\"\" One-hot encode specified columns in the DataFrame and return the indices of new columns. \"\"\"\n",
    "    original_columns = set(df.columns)\n",
    "    \n",
    "    for column in columns:\n",
    "        # Get one-hot encoded DataFrame for the current column\n",
    "        encoded = pd.get_dummies(df[column], prefix=column)\n",
    "\n",
    "        # Concatenate with original DataFrame and drop the original column\n",
    "        df = pd.concat([df, encoded], axis=1).drop(column, axis=1)\n",
    "    \n",
    "    # Determine new columns added\n",
    "    new_columns = set(df.columns) - original_columns\n",
    "\n",
    "    # Determine indices of the new columns\n",
    "    new_column_indices = [df.columns.get_loc(c) for c in new_columns]\n",
    "\n",
    "    return df, new_column_indices\n",
    "\n",
    "def scale_columns(df, scaler, columns):\n",
    "    \"\"\" Scale specified columns using the given scaler \"\"\"\n",
    "    df[columns] = scaler.fit_transform(df[columns])\n",
    "    return df\n",
    "\n",
    "# Function to calculate the ratio of the most common value\n",
    "def most_common_ratio(series):\n",
    "    return series.value_counts(normalize=True).iloc[0]\n",
    "\n",
    "\n",
    "def feature_engineering_C_part1(df, target_columns):\n",
    "    columns_to_one_hot_encode = [\"architectural_archetype\", \"stories\", \"soil_class\", \"seismic_zone\", \"connection_system\", 'Story', 'Direction', 'Wall']\n",
    "    columns_to_scale = ['L cm', 'xi cm', 'yi cm', 'D+0.25L', 'Story Area']\n",
    "    \n",
    "    target_data = df[target_columns]\n",
    "    df = df.drop(columns=target_columns)\n",
    "    \n",
    "    # One-hot encode specified columns\n",
    "    df,new_column_indices = one_hot_encode_columns(df, columns_to_one_hot_encode)\n",
    "\n",
    "    for col in df.columns:\n",
    "        if not is_numeric_dtype(df[col]):\n",
    "            df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "    \n",
    "    # Scale specified columns\n",
    "    scaler = MinMaxScaler()\n",
    "    df = scale_columns(df, scaler, columns_to_scale)\n",
    "    \n",
    "    #Removes columns with zero variance\n",
    "    df = df.loc[:, df.var() != 0]\n",
    "    \n",
    "    df = pd.concat([df, target_data], axis=1)\n",
    "    return df\n",
    "\n",
    "def feature_engineering_C_part2(df, target_columns):\n",
    "    columns_to_one_hot_encode = [\"architectural_archetype\", \"stories\", \"soil_class\", \"seismic_zone\", \"connection_system\"]\n",
    "    \n",
    "    target_data = df[target_columns]\n",
    "    df = df.drop(columns=target_columns)\n",
    "    \n",
    "    # One-hot encode specified columns\n",
    "    df, new_column_indices = one_hot_encode_columns(df, columns_to_one_hot_encode)\n",
    "    \n",
    "    indicator_dict = {}\n",
    "\n",
    "    # Creating indicator variables only for columns with NaN values\n",
    "    for col in df.columns:\n",
    "        if df[col].isna().any():\n",
    "            indicator_col_name = f\"{col}_present\"\n",
    "            indicator_dict[indicator_col_name] = df[col].notna().astype(int)\n",
    "\n",
    "    # Create a DataFrame from the dictionary and concatenate it\n",
    "    indicator_df = pd.DataFrame(indicator_dict)\n",
    "    df = pd.concat([df, indicator_df], axis=1)\n",
    "    \n",
    "    # Custom Imputation\n",
    "    for col in df.columns:\n",
    "        if df[col].isna().mean() > 0.40:\n",
    "            df[col].fillna(0, inplace=True)\n",
    "        else:\n",
    "            if is_numeric_dtype(df[col]):\n",
    "                df[col].fillna(df[col].median(), inplace=True)\n",
    "\n",
    "    # Ensuring all columns are numeric after imputation\n",
    "    for col in df.columns:\n",
    "        if not is_numeric_dtype(df[col]):\n",
    "            df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "\n",
    "    all_columns = set(df.columns)\n",
    "    non_scale_columns = set(columns_to_one_hot_encode)\n",
    "    columns_to_scale = list(all_columns - non_scale_columns) \n",
    "    \n",
    "    # Scale specified columns\n",
    "    scaler = MinMaxScaler()\n",
    "    df = scale_columns(df, scaler, columns_to_scale)\n",
    "    \n",
    "    #Removes columns with zero variance\n",
    "    df = df.loc[:, df.var() != 0]\n",
    "    \n",
    "    # Removing highly correlated columns\n",
    "    corr_matrix = df.corr().abs()\n",
    "    upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
    "\n",
    "    # Find index of feature columns with correlation greater than 0.90\n",
    "    to_drop = [column for column in upper.columns if any(upper[column] > 0.90)]\n",
    "    df = df.drop(columns=to_drop)\n",
    "    \n",
    "    # Threshold for dropping columns\n",
    "    threshold = 0.95\n",
    "\n",
    "    # Apply the function and drop columns based on the threshold\n",
    "    cols_to_drop = [col for col in df.columns if most_common_ratio(df[col]) > threshold]\n",
    "    df_dropped = df.drop(columns=cols_to_drop)\n",
    "    \n",
    "    df = pd.concat([df_dropped, target_data], axis=1)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def feature_engineering_D(df, target_columns):\n",
    "    columns_to_one_hot_encode = [\"architectural_archetype\", \"stories\", \"soil_class\", \"seismic_zone\", \"connection_system\"]\n",
    "    \n",
    "    target_data = df[target_columns]\n",
    "    df = df.drop(columns=target_columns)\n",
    "    \n",
    "    # One-hot encode specified columns\n",
    "    df, new_column_indices = one_hot_encode_columns(df, columns_to_one_hot_encode)\n",
    "    \n",
    "    indicator_dict = {}\n",
    "\n",
    "    # Creating indicator variables only for columns with NaN values\n",
    "    for col in df.columns:\n",
    "        if df[col].isna().any():\n",
    "            indicator_col_name = f\"{col}_present\"\n",
    "            indicator_dict[indicator_col_name] = df[col].notna().astype(int)\n",
    "\n",
    "    # Create a DataFrame from the dictionary and concatenate it\n",
    "    indicator_df = pd.DataFrame(indicator_dict)\n",
    "    df = pd.concat([df, indicator_df], axis=1)\n",
    "    \n",
    "    # Custom Imputation\n",
    "    for col in df.columns:\n",
    "        if df[col].isna().mean() > 0.40:\n",
    "            df[col].fillna(0, inplace=True)\n",
    "        else:\n",
    "            if is_numeric_dtype(df[col]):\n",
    "                df[col].fillna(df[col].median(), inplace=True)\n",
    "\n",
    "    # Ensuring all columns are numeric after imputation\n",
    "    for col in df.columns:\n",
    "        if not is_numeric_dtype(df[col]):\n",
    "            df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "\n",
    "    all_columns = set(df.columns)\n",
    "    non_scale_columns = set(columns_to_one_hot_encode)\n",
    "    columns_to_scale = list(all_columns - non_scale_columns) \n",
    "    \n",
    "    # Scale specified columns\n",
    "    scaler = MinMaxScaler()\n",
    "    df = scale_columns(df, scaler, columns_to_scale)\n",
    "    \n",
    "    #Removes columns with zero variance\n",
    "    df = df.loc[:, df.var() != 0]\n",
    "    \n",
    "    # Removing highly correlated columns\n",
    "    corr_matrix = df.corr().abs()\n",
    "    upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
    "\n",
    "    # Find index of feature columns with correlation greater than 0.90\n",
    "    to_drop = [column for column in upper.columns if any(upper[column] > 0.90)]\n",
    "    df = df.drop(columns=to_drop)\n",
    "    \n",
    "    # Threshold for dropping columns\n",
    "    threshold = 0.95\n",
    "\n",
    "    # Apply the function and drop columns based on the threshold\n",
    "    cols_to_drop = [col for col in df.columns if most_common_ratio(df[col]) > threshold]\n",
    "    df_dropped = df.drop(columns=cols_to_drop)\n",
    "    \n",
    "    df = pd.concat([df_dropped, target_data], axis=1)\n",
    "    \n",
    "    return df\n",
    "\n",
    "#Do Feature Engineering for C part 1\n",
    "path ='./Files/Before_Feature_Engineering'\n",
    "path_FE ='Files/After_Feature_Engineering/'\n",
    "file_path_C_part1 = path + '/data_C_part1.csv'\n",
    "df_C_part1 = pd.read_csv(file_path_C_part1, low_memory=False)\n",
    "target_column_C = [\"Nail spacing [cm]\", \"Number sheathing panels\", \"Number end studs\", \"Total number studs\",\"HoldDown Model / ATS\"]\n",
    "df_C_part1 = feature_engineering_C_part1(df_C_part1, target_column_C) \n",
    "\n",
    "prepared_file_path_C_part1 = path_FE + 'data_C_part1_FE.csv'\n",
    "df_C_part1.to_csv(prepared_file_path_C_part1, index=False)\n",
    "\n",
    "\n",
    "#Do Feature Engineering for C part 2\n",
    "file_path_C_part2 = path +'/data_C_part2.csv'\n",
    "df_C_part2 = pd.read_csv(file_path_C_part2, low_memory=False)\n",
    "target_column_C_2 = ['Tx(s)', 'Ty(s)']\n",
    "df_C_part2 = feature_engineering_C_part2(df_C_part2, target_column_C_2) \n",
    "\n",
    "prepared_file_path_C_part2 = path_FE + 'data_C_part2_FE.csv'\n",
    "df_C_part2.to_csv(prepared_file_path_C_part2, index=False)\n",
    "\n",
    "#Do Feature Engineering for D\n",
    "file_path_D = path +'/data_D.csv'\n",
    "df_D = pd.read_csv(file_path_D, low_memory=False)\n",
    "target_column_D = ['Ωx', 'Ωy', 'µx', 'µy', 'CMR', 'SSF', 'ACMR', 'IO-ln θ','IO-β','LS-ln θ','LS-β', 'CP-ln θ','CP-β']\n",
    "df_D = feature_engineering_D(df_D, target_column_D) \n",
    "\n",
    "prepared_file_path_D = path_FE + 'data_D_FE.csv'\n",
    "df_D.to_csv(prepared_file_path_D, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbbf5f39",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_C_part1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dee95929",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_C_part2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8be12a70",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d519c28",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
