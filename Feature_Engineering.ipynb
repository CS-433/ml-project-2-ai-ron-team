{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f7f0fa58",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from pandas.api.types import is_numeric_dtype\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from pandas.api.types import is_numeric_dtype\n",
    "\n",
    "def one_hot_encode_columns(df, columns):\n",
    "    \"\"\"\n",
    "    One-hot encode specified columns in a DataFrame.\n",
    "\n",
    "    :param df: The DataFrame to process.\n",
    "    :param columns: A list of column names to be one-hot encoded.\n",
    "    :return: The modified DataFrame and a list of new column indices.\n",
    "    \"\"\"\n",
    "    original_columns = set(df.columns)\n",
    "    for column in columns:\n",
    "        encoded = pd.get_dummies(df[column], prefix=column)\n",
    "        df = pd.concat([df, encoded], axis=1).drop(column, axis=1)\n",
    "    new_columns = set(df.columns) - original_columns\n",
    "    new_column_indices = [df.columns.get_loc(c) for c in new_columns]\n",
    "    return df, new_column_indices\n",
    "\n",
    "def scale_columns(df, scaler, columns):\n",
    "    \"\"\"\n",
    "    Scale specified columns using the provided scaler.\n",
    "\n",
    "    :param df: The DataFrame to process.\n",
    "    :param scaler: Scaler instance (e.g., MinMaxScaler).\n",
    "    :param columns: A list of column names to be scaled.\n",
    "    :return: The modified DataFrame.\n",
    "    \"\"\"\n",
    "    df[columns] = scaler.fit_transform(df[columns])\n",
    "    return df\n",
    "\n",
    "def most_common_ratio(series):\n",
    "    \"\"\"\n",
    "    Calculate the ratio of the most common value in a Series.\n",
    "\n",
    "    :param series: Pandas Series to analyze.\n",
    "    :return: Ratio of the most common value.\n",
    "    \"\"\"\n",
    "    return series.value_counts(normalize=True).iloc[0]\n",
    "\n",
    "def remove_column_with_high_ratio(df, exclude_columns):\n",
    "    \"\"\"\n",
    "    Remove columns with a high ratio of a single value.\n",
    "\n",
    "    :param df: The DataFrame to process.\n",
    "    :param exclude_columns: A list of column names to be excluded from processing.\n",
    "    :return: The modified DataFrame.\n",
    "    \"\"\"\n",
    "    threshold = 0.98\n",
    "    cols_to_drop = [col for col in df.columns if most_common_ratio(df[col]) > threshold and col not in exclude_columns]\n",
    "    return df.drop(columns=cols_to_drop)\n",
    "\n",
    "def creating_indicator_variables(df):\n",
    "    \"\"\"\n",
    "    Create indicator variables for columns with missing values.\n",
    "\n",
    "    :param df: The DataFrame to process.\n",
    "    :return: The modified DataFrame.\n",
    "    \"\"\"\n",
    "    indicator_dict = {}\n",
    "    for col in df.columns:\n",
    "        if df[col].isna().any():\n",
    "            indicator_col_name = f\"{col}_present\"\n",
    "            indicator_dict[indicator_col_name] = df[col].notna().astype(int)\n",
    "    indicator_df = pd.DataFrame(indicator_dict)\n",
    "    return pd.concat([df, indicator_df], axis=1)\n",
    "\n",
    "def custom_imputation(df):\n",
    "    \"\"\"\n",
    "    Apply custom imputation to DataFrame columns based on the percentage of missing values.\n",
    "\n",
    "    :param df: The DataFrame to process.\n",
    "    :return: The modified DataFrame.\n",
    "    \"\"\"\n",
    "    for col in df.columns:\n",
    "        if df[col].isna().mean() > 0.40:\n",
    "            df[col].fillna(0, inplace=True)\n",
    "        elif is_numeric_dtype(df[col]):\n",
    "            df[col].fillna(df[col].median(), inplace=True)\n",
    "    return df\n",
    "\n",
    "def remove_highly_correlated_column(df, exclude_columns):\n",
    "    \"\"\"\n",
    "    Remove highly correlated columns from the DataFrame.\n",
    "\n",
    "    :param df: The DataFrame to process.\n",
    "    :param exclude_columns: A list of column names to be excluded from processing.\n",
    "    :return: The modified DataFrame.\n",
    "    \"\"\"\n",
    "    threshold = 0.99\n",
    "    corr_matrix = df.drop(columns=exclude_columns).corr().abs()\n",
    "    upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
    "    to_drop = [column for column in upper.columns if any(upper[column] > threshold) and column not in exclude_columns]\n",
    "    return df.drop(columns=to_drop)\n",
    "\n",
    "def remove_column_with_zero_variance(df):\n",
    "    \"\"\"\n",
    "    Remove columns with zero variance from the DataFrame.\n",
    "\n",
    "    :param df: The DataFrame to process.\n",
    "    :return: The modified DataFrame.\n",
    "    \"\"\"\n",
    "    return df.loc[:, df.var() != 0]\n",
    "\n",
    "def ensure_column_are_numeric(df):\n",
    "    \"\"\"\n",
    "    Ensure all columns in the DataFrame are of numeric type.\n",
    "\n",
    "    :param df: The DataFrame to process.\n",
    "    :return: The modified DataFrame.\n",
    "    \"\"\"\n",
    "    for col in df.columns:\n",
    "        if not is_numeric_dtype(df[col]):\n",
    "            df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "    return df\n",
    "\n",
    "def impute_with_median(df):\n",
    "    \"\"\"\n",
    "    Impute missing values in the DataFrame with the median.\n",
    "\n",
    "    :param df: The DataFrame to process.\n",
    "    :return: The modified DataFrame.\n",
    "    \"\"\"\n",
    "    non_empty_columns = df.columns[df.notna().any()].tolist()\n",
    "    imputer = SimpleImputer(strategy='median')\n",
    "    df = imputer.fit_transform(df)\n",
    "    return pd.DataFrame(df, columns=non_empty_columns)\n",
    "\n",
    "def impute_with_mean(df):\n",
    "    \"\"\"\n",
    "    Impute missing values in the DataFrame with the median.\n",
    "\n",
    "    :param df: The DataFrame to process.\n",
    "    :return: The modified DataFrame.\n",
    "    \"\"\"\n",
    "    non_empty_columns = df.columns[df.notna().any()].tolist()\n",
    "    imputer = SimpleImputer(strategy='mean')\n",
    "    df = imputer.fit_transform(df)\n",
    "    return pd.DataFrame(df, columns=non_empty_columns)\n",
    "\n",
    "def impute_with_zero(df):\n",
    "    \"\"\"\n",
    "    Impute missing values in the DataFrame with the median.\n",
    "\n",
    "    :param df: The DataFrame to process.\n",
    "    :return: The modified DataFrame.\n",
    "    \"\"\"\n",
    "    non_empty_columns = df.columns[df.notna().any()].tolist()\n",
    "    df = df[non_empty_columns]\n",
    "    imputer = SimpleImputer(strategy='constant', fill_value=0)\n",
    "    df = imputer.fit_transform(df)\n",
    "    return pd.DataFrame(df, columns=non_empty_columns)\n",
    "\n",
    "def determine_columns_to_scale(df, columns_to_one_hot_encode):\n",
    "    \"\"\"\n",
    "    Determine which columns in the DataFrame should be scaled.\n",
    "\n",
    "    :param df: The DataFrame to process.\n",
    "    :param columns_to_one_hot_encode: A list of column names that should not be scaled.\n",
    "    :return: A list of column names to be scaled.\n",
    "    \"\"\"\n",
    "    all_columns = set(df.columns)\n",
    "    non_scale_columns = set(columns_to_one_hot_encode)\n",
    "    return list(all_columns - non_scale_columns)\n",
    "\n",
    "def feature_engineering_C_part1(df, target_columns):\n",
    "    \"\"\"\n",
    "    Perform feature engineering on the DataFrame for part C-1.\n",
    "\n",
    "    :param df: The DataFrame to process.\n",
    "    :param target_columns: A list of target column names.\n",
    "    :return: The modified DataFrame.\n",
    "    \"\"\"\n",
    "    columns_to_one_hot_encode = [\"architectural_archetype\", \"stories\", \"soil_class\", \"seismic_zone\", \"connection_system\", 'Story', 'Direction', 'Wall']\n",
    "    columns_to_scale = ['L cm', 'xi cm', 'yi cm', 'D+0.25L', 'Story Area']\n",
    "    \n",
    "    target_data = df[target_columns]\n",
    "    df = df.drop(columns=target_columns)\n",
    "    df,new_column_indices = one_hot_encode_columns(df, columns_to_one_hot_encode)\n",
    "    ensure_column_are_numeric(df)\n",
    "    columns_to_scale\n",
    "    scale_columns(df, MinMaxScaler(), columns_to_scale)\n",
    "    df = remove_column_with_zero_variance(df)\n",
    "    df = pd.concat([df, target_data], axis=1)\n",
    "    return df\n",
    "\n",
    "def feature_engineering2(df, target_columns):\n",
    "    \"\"\"\n",
    "    Perform feature engineering on the DataFrame for part C-2 and D.\n",
    "\n",
    "    :param df: The DataFrame to process.\n",
    "    :param target_columns: A list of target column names.\n",
    "    :return: The modified DataFrame.\n",
    "    \"\"\"\n",
    "    columns_to_one_hot_encode = [\"architectural_archetype\", \"stories\", \"soil_class\", \"seismic_zone\", \"connection_system\"]\n",
    "    target_data = df[target_columns]\n",
    "    df = df.drop(columns=target_columns)\n",
    "    \n",
    "    # One-hot encode specified columns\n",
    "    df, new_column_indices = one_hot_encode_columns(df, columns_to_one_hot_encode)\n",
    "    \n",
    "    df = impute_with_zero(df)\n",
    "    ensure_column_are_numeric(df)\n",
    "    df = scale_columns(df, MinMaxScaler(), determine_columns_to_scale(df, columns_to_one_hot_encode))\n",
    "    \n",
    "    df = remove_column_with_zero_variance(df)\n",
    "    exclude_columns = [\"connection_system_ATS\", \"connection_system_HD\"]\n",
    "    df = remove_highly_correlated_column(df, exclude_columns)\n",
    "    df_dropped = remove_column_with_high_ratio(df, exclude_columns)\n",
    "    df = pd.concat([df_dropped, target_data], axis=1)\n",
    "    \n",
    "    return df\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "def read_and_process_data(file_path, target_columns, feature_engineering_func):\n",
    "    \"\"\"\n",
    "    Read data from a CSV file, apply feature engineering, and return the processed DataFrame.\n",
    "\n",
    "    :param file_path: Path to the CSV file.\n",
    "    :param target_columns: List of target columns for feature engineering.\n",
    "    :param feature_engineering_func: Function to be used for feature engineering.\n",
    "    :return: Processed DataFrame.\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(file_path, low_memory=False)\n",
    "    return feature_engineering_func(df, target_columns)\n",
    "\n",
    "def save_processed_data(df, file_path):\n",
    "    \"\"\"\n",
    "    Save the processed DataFrame to a CSV file.\n",
    "\n",
    "    :param df: DataFrame to be saved.\n",
    "    :param file_path: Path where the DataFrame will be saved.\n",
    "    \"\"\"\n",
    "    df.to_csv(file_path, index=False)\n",
    "\n",
    "def main():\n",
    "    # Paths for source data and processed data\n",
    "    path = './Files/Before_Feature_Engineering'\n",
    "    path_FE = 'Files/After_Feature_Engineering/'\n",
    "\n",
    "    # Feature Engineering for C part 1\n",
    "    file_path_C_part1 = path + '/data_C_part1.csv'\n",
    "    target_column_C = [\"Nail spacing [cm]\", \"Number sheathing panels\", \"Number end studs\", \"Total number studs\", \"HoldDown Model / ATS\"]\n",
    "    df_C_part1 = read_and_process_data(file_path_C_part1, target_column_C, feature_engineering_C_part1)\n",
    "    save_processed_data(df_C_part1, path_FE + 'data_C_part1_FE.csv')\n",
    "\n",
    "    # Feature Engineering for C part 2\n",
    "    file_path_C_part2 = path + '/data_C_part2.csv'\n",
    "    target_column_C_2 = ['Tx(s)', 'Ty(s)']\n",
    "    df_C_part2 = read_and_process_data(file_path_C_part2, target_column_C_2, feature_engineering2)\n",
    "    save_processed_data(df_C_part2, path_FE + 'data_C_part2_FE.csv')\n",
    "\n",
    "    # Feature Engineering for D\n",
    "    file_path_D = path + '/data_D.csv'\n",
    "    target_column_D = ['Ωx', 'Ωy', 'µx', 'µy', 'CMR', 'SSF', 'ACMR', 'IO-ln θ', 'IO-β', 'LS-ln θ', 'LS-β', 'CP-ln θ', 'CP-β']\n",
    "    df_D = read_and_process_data(file_path_D, target_column_D, feature_engineering2)\n",
    "    save_processed_data(df_D, path_FE + 'data_D_FE.csv')\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
