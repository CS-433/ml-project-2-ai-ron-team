{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4af96992",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Given the explanation of the header, we will define a function to parse it and extract the relevant information.\n",
    "def parse_header(header):\n",
    "    # Splitting the header string by underscores\n",
    "    parts = header.split('_')\n",
    "    \n",
    "    # Extracting the information based on the index\n",
    "    architectural_archetype = parts[0]  # C\n",
    "    stories = int(parts[1])            # 3\n",
    "    soil_class = parts[4]              # D\n",
    "    seismic_zone = int(parts[6])       # 3\n",
    "    connection_system = parts[8]       # HD\n",
    "    \n",
    "    # Creating a dictionary to hold the extracted information\n",
    "    extracted_info = {\n",
    "        \"architectural_archetype\": architectural_archetype,\n",
    "        \"stories\": stories,\n",
    "        \"soil_class\": soil_class,\n",
    "        \"seismic_zone\": seismic_zone,\n",
    "        \"connection_system\": connection_system\n",
    "    }\n",
    "    \n",
    "    return extracted_info\n",
    "\n",
    "\n",
    "# We have an issue where in an excel file if a column is represented by multiple rows, the first one will have the correct value and then the others will have nan values\n",
    "def fill_values_based_on_key(data, key_column_index, value_row_index, finishing_row_informationB):\n",
    "    \"\"\"\n",
    "    Fill the values in the value column based on the last non-NaN value in the key column.\n",
    "\n",
    "    :param data: The 2D list (list of lists) representing the data.\n",
    "    :param key_column_index: The index of the column to use as the key.\n",
    "    :param value_row_index: The index of the row where values are to be filled.\n",
    "    :return: None; the operation modifies the data list in place.\n",
    "    \"\"\"\n",
    "    last_valid_key = None\n",
    "    for i in range(value_row_index, finishing_row_informationB):\n",
    "        key_value = data.iat[i, key_column_index]\n",
    "        if pd.notna(key_value):  # Check if the key column value is not NaN\n",
    "            last_valid_key = key_value\n",
    "        if pd.isna(key_value):  # Check if the value column is NaN\n",
    "            data.iat[i, key_column_index] = last_valid_key\n",
    "\n",
    "\n",
    "\n",
    "def prepare_data_to_csv(file_path, sheet_name) :\n",
    "    \n",
    "    # Load the Excel file\n",
    "    # For this file we will read it with a header since it is easier to extract the information of type A\n",
    "    data = pd.read_excel(file_path, sheet_name=sheet_name)\n",
    "    \n",
    "    # To find the number of buildings, we need to find how many unamed columns there are\n",
    "    nbr_building = len([col for col in data.columns if not 'Unnamed' in str(col)])\n",
    "\n",
    "    #general parameters to understand better to not have random parameters\n",
    "    nbr_story = 5 #this parameter is not used in my code but maybe in another\n",
    "    starting_row_informationB = 14 #normally same for all files\n",
    "    size_columns_informationA = 5 #normally same for all files\n",
    "    row_Tx_Ty_values = 12\n",
    "\n",
    "\n",
    "    # Extract Type A Information\n",
    "\n",
    "    # Step 1: Parsing headers and store the results\n",
    "    parsed_data = []\n",
    "    for i in range(1, nbr_building+1):\n",
    "        header = data[i][1]\n",
    "        parsed_data.append(parse_header(header))\n",
    "\n",
    "    # Step 2: Defining the desired columns\n",
    "    columns = [\"architectural_archetype\", \"stories\", \"soil_class\", \"seismic_zone\"]\n",
    "\n",
    "    # Step 3: Create a new table and populate it with the parsed data\n",
    "    new_table = []\n",
    "    for item in parsed_data:\n",
    "        row = [item[col] for col in columns]\n",
    "        new_table.append(row)\n",
    "\n",
    "    # 'new_table' now contains the parsed data structured as rows and columns\n",
    "    df = pd.DataFrame(new_table, columns=columns)\n",
    "\n",
    "\n",
    "    data2 = pd.read_excel(file_path,sheet_name=sheet_name, header=None)\n",
    "    \n",
    "    # We want to find the finishing row of the tables (we don't know sine it is an excel file without headers)\n",
    "    # We want to consider rows that have at least one non-NaN value:\n",
    "    finishing_row_informationB = data.dropna(how='all').index[-1] + 1\n",
    "\n",
    "\n",
    "    # Assuming data2 is a list of lists representation of your Excel data\n",
    "    fill_values_based_on_key(data2, 3, starting_row_informationB, finishing_row_informationB)\n",
    "    fill_values_based_on_key(data2, 4, starting_row_informationB, finishing_row_informationB)\n",
    "\n",
    "\n",
    "    repetitions = finishing_row_informationB - starting_row_informationB\n",
    "\n",
    "    #Add type A information\n",
    "    # Repeat each row in the DataFrame\n",
    "    repeated_df = pd.DataFrame(np.repeat(df.values, repetitions, axis=0), columns=df.columns)\n",
    "\n",
    "    #Add type B information part 1\n",
    "    # Replicate df1 twelve times, ensuring the column names are as expected.\n",
    "    df1 = [data2.iloc[starting_row_informationB:finishing_row_informationB, 3:8]\n",
    "           .rename(columns={data2.columns[3]: \"Story\",\n",
    "                                                    data2.columns[4]: \"Direction\",\n",
    "                                                    data2.columns[5]: \"Wall\",\n",
    "                                                    data2.columns[6]: \"L [cm]\",\n",
    "                                                    data2.columns[7]: \"xi [cm]\"}) for _ in range(nbr_building)]\n",
    "    #Add type C information\n",
    "    # Create subsets using list comprehensions for dfs with the correct column names\n",
    "    dfs = [data2.iloc[starting_row_informationB:finishing_row_informationB,\n",
    "                      9 + size_columns_informationA * i : 13 + size_columns_informationA * i]\n",
    "           .rename(columns={data2.columns[9 + size_columns_informationA * i]: \"Nail spacing [cm]\",\n",
    "                   data2.columns[10 + size_columns_informationA * i]: \"Number sheathing panels\",\n",
    "                   data2.columns[11 + size_columns_informationA * i]: \"Number end studs\",\n",
    "                   data2.columns[12 + size_columns_informationA * i]: \"Total number studs\"}) for i in range(nbr_building)]\n",
    "\n",
    "\n",
    "    # Concatenate lists of DataFrames, ensuring the column names align.\n",
    "    result2 = pd.concat(df1, ignore_index=True)\n",
    "    result3 = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "\n",
    "    #We have to add additional type B information (part 2)\n",
    "    #We will also add additional type C information (part2)\n",
    "    # Initialize empty lists to store the D+0.25L and Story Area values\n",
    "    d_plus_quarter_l_values = []\n",
    "    story_area_values = []\n",
    "    Tx_values = []\n",
    "    Ty_values = []\n",
    "\n",
    "    # Loop through the specified ranges and compute the values\n",
    "    for i in range(0, nbr_building):\n",
    "        for j in range(0, finishing_row_informationB-starting_row_informationB):\n",
    "            # Extract D+0.25L value and append to the list \n",
    "            story = int(result2.iat[j, 0])\n",
    "\n",
    "            d_plus_quarter_l =  data2.iat[4 + story, 11 + size_columns_informationA * i]\n",
    "            d_plus_quarter_l_values.append(d_plus_quarter_l)\n",
    "\n",
    "            # Extract Story Area value and append to the list\n",
    "            story_area = data2.iat[4 + story, 13 + size_columns_informationA * i]\n",
    "            story_area_values.append(story_area)\n",
    "\n",
    "            Tx_values.append(data2.iat[row_Tx_Ty_values, 9 + size_columns_informationA * i])\n",
    "            Ty_values.append(data2.iat[row_Tx_Ty_values, 10 + size_columns_informationA * i])\n",
    "\n",
    "\n",
    "    # Add the new columns to result2\n",
    "    result2['D+0.25L'] = d_plus_quarter_l_values\n",
    "    result2['Story Area'] = story_area_values\n",
    "    result3['Tx(s)'] = Tx_values\n",
    "    result3['Ty(s)'] = Ty_values\n",
    "\n",
    "    # Concatenate horizontally, ensuring that both dataframes have the same index\n",
    "    resultFinal = pd.concat([repeated_df, result2, result3], axis=1, ignore_index=False)\n",
    "    \n",
    "    \n",
    "    #Creating another table if we are using only information of type A to predict Tx and Ty not everything\n",
    "    Tx_values = []\n",
    "    Ty_values = []\n",
    "\n",
    "    # Loop through the specified ranges and compute the values\n",
    "    for i in range(0, nbr_building): \n",
    "            Tx_values.append(data2.iat[row_Tx_Ty_values, 9 + size_columns_informationA * i])\n",
    "            Ty_values.append(data2.iat[row_Tx_Ty_values, 10 + size_columns_informationA * i])\n",
    "\n",
    "    df['Tx(s)'] = Tx_values\n",
    "    df['Ty(s)'] = Ty_values\n",
    "\n",
    "    return resultFinal, df\n",
    "\n",
    "\n",
    "# For each file, file_path sheet (page of the excel file)\n",
    "# List of Excel files\n",
    "files = ['./Design_P_ATS.xlsx', './Design_P_HD.xlsx',\n",
    "         './Design_D_ATS.xlsx', './Design_D_HD.xlsx',\n",
    "         './Design_C_ATS.xlsx', './Design_C_HD.xlsx',\n",
    "         './Design_Q_ATS.xlsx', './Design_Q_HD.xlsx']\n",
    "\n",
    "# Initialize an empty list to store file and sheet index information\n",
    "Files_informations = []\n",
    "\n",
    "# For each file, get all sheet names and add to Files_informations\n",
    "for file in files:\n",
    "    xls = pd.ExcelFile(file)\n",
    "    sheet_names = xls.sheet_names\n",
    "    for sheet_index, sheet_name in enumerate(sheet_names):\n",
    "        Files_informations.append([file, sheet_index])\n",
    "\n",
    "resultsFinal = []\n",
    "resultsDF = []\n",
    "for i in range(len(Files_informations)):\n",
    "    resultFinal, df = prepare_data_to_csv(Files_informations[i][0], Files_informations[i][1])\n",
    "    resultsFinal.append(resultFinal)\n",
    "    resultsDF.append(df)\n",
    "\n",
    "resultFinal = pd.concat(resultsFinal, axis=0, ignore_index=True)\n",
    "prepared_file_path = 'prepared_data_all.csv'\n",
    "resultFinal.to_csv(prepared_file_path, index=False)\n",
    "\n",
    "df = pd.concat(resultsDF, axis=0, ignore_index=True)\n",
    "prepared_file_path2 = 'prepared_data_all_part2.csv'\n",
    "df.to_csv(prepared_file_path2, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b40a7e3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
